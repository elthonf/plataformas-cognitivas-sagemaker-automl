{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdu√ß√£o\n",
    "\n",
    "Word2Vec √© um algoritmo bem popular usado para gerar representa√ß√µes vetoriais densas de palavras em grandes textos usando aprendizado n√£o supervisionado. Essas representa√ß√µes s√£o √∫teis para muitas tarefas de processamento de linguagem natural (PLN/NLP), como an√°lise de sentimentos, reconhecimento de entidades nomeadas e tradu√ß√£o autom√°tica.\n",
    "\n",
    "\n",
    "Modelos populares que aprendem tais representa√ß√µes ignoram a morfologia das palavras, atribuindo um vetor distinto a cada palavra. Essa √© uma limita√ß√£o, especialmente para idiomas com vocabul√°rios grandes e muitas palavras incomuns. O algoritmo *SageMaker BlazingText* pode aprender representa√ß√µes vetoriais associadas aos caracteres n-gramas; representando palavras como a soma dessas representa√ß√µes de n-gramas de caracteres [1]. Esse m√©todo permite que o *BlazingText* gere vetores para palavras fora do vocabul√°rio (out-of-vocabulary / OOV), conforme demonstrado neste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ferramentas populares como [FastText](https://github.com/facebookresearch/fastText) aprendem a incorpora√ß√£o de subpalavras para gerar representa√ß√µes, mas t√™m uma escala ruim, pois podem ser executadas apenas em CPUs. O BlazingText estende o modelo FastText para alavancar GPUs, fornecendo mais de 10x acelera√ß√£o, dependendo do hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] P. Bojanowski, E. Grave, A. Joulin, T. Mikolov, [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configura√ß√µes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "isConfigCell": true
   },
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role) # Pega a Role/Fun√ß√£o que criamos e que o Notetbook est√° rodando\n",
    "\n",
    "bucket = sess.default_bucket() # Pega o bucket padr√£o da Sess√£o (ou coloque aqui teu bucket j√° usado)\n",
    "print(bucket)\n",
    "prefix = 'blazingtext/aulasagemaker' #Pasta em que todo o trabalho ser√° executado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leitura dos dados para treinamento\n",
    "\n",
    "O BlazingText espera um √∫nico arquivo de texto pr√©-processado com tokens separados por espa√ßo e cada linha do arquivo deve conter uma √∫nica frase. Eu j√° deixei o arquivo \n",
    "\n",
    "Neste exemplo, vamos treinar os vetores no conjunto de dados [text8](http://mattmahoney.net/dc/textdata.html) (100 MB), que √© uma vers√£o pequena (j√° pr√©-processada) do dump da Wikipedia em ingl√™s. O arquivo j√° est√° pronto na pasta `datasets/textmining/text8.gz` e precisa ser descompactado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncompressing\n",
    "!gzip -d datasets/textmining/text8.gz -f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ap√≥s a descompactar o arquivo, precisamos carreg√°-los no S3 para que possam ser consumidos pelo SageMaker para o treinamento. Usaremos o Python SDK para fazer upload desse arquivo no bucket e na pasta escolhidos acima.\n",
    "\n",
    "Execute esta c√©lula e confira se o arquivo est√° no local esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_channel = prefix + '/train'\n",
    "\n",
    "sess.upload_data(path='datasets/textmining/text8', bucket=bucket, key_prefix=train_channel)\n",
    "\n",
    "s3_train_data = 's3://{}/{}'.format(bucket, train_channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ent√£o, precisamos configurar um local de sa√≠da no S3, onde o arquivo do modelo ser√° salvo. Importante: √© a√≠ que ficar√° o arquivo resultante do treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print(s3_output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o treino  \n",
    "\n",
    "Agora que terminamos todas as configura√ß√µes necess√°rias, estamos prontos para treinar nosso detector de objetos. Para come√ßar, vamos criar um objeto ``sageMaker.estimator.Estimator``. Este estimador iniciar√° o trabalho de treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_name = boto3.Session().region_name\n",
    "print(region_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.amazon.amazon_estimator.get_image_uri(region_name, \"blazingtext\", \"latest\")\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o modelo BlazingText para gerar vetores de palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model = sagemaker.estimator.Estimator(container,\n",
    "                                         role, \n",
    "                                         train_instance_count=1, \n",
    "                                         train_instance_type='ml.c4.2xlarge', # Use of ml.p3.2xlarge is highly recommended for highest speed and cost efficiency\n",
    "                                         train_volume_size = 30,\n",
    "                                         train_max_run = 360000,\n",
    "                                         input_mode= 'File',\n",
    "                                         output_path=s3_output_location,\n",
    "                                         sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste dos hyperpar√¢metros.  \n",
    "Consulte a [algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext_hyperparameters.html) para a lista completa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.set_hyperparameters(mode=\"skipgram\",\n",
    "                             epochs=5,\n",
    "                             min_count=5,\n",
    "                             sampling_threshold=0.0001,\n",
    "                             learning_rate=0.05,\n",
    "                             window_size=5,\n",
    "                             vector_dim=100,\n",
    "                             negative_samples=5,\n",
    "                             subwords=True, # Enables learning of subword embeddings for OOV word vector generation\n",
    "                             min_char=3, # min length of char ngrams\n",
    "                             max_char=6, # max length of char ngrams\n",
    "                             batch_size=11, #  = (2*window_size + 1) (Preferred. Used only if mode is batch_skipgram)\n",
    "                             evaluation=True)# Perform similarity evaluation on WS-353 dataset at the end of training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepara vari√°vel que possui os dados que ser√£o usados no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(s3_train_data, distribution='FullyReplicated', \n",
    "                        content_type='text/plain', s3_data_type='S3Prefix')\n",
    "data_channels = {'train': train_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois que o trabalho terminar, uma mensagem \"Trabalho conclu√≠do\" ser√° impressa. O modelo treinado pode ser encontrado no bucket S3 que foi configurado como `output_path` no estimador.\n",
    "\n",
    "Este processo deve demorar de **7 a 10 minutos** ... podemos acompanhar  pelo portal tamb√©m üòä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_model.fit(inputs=data_channels, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hosting / Endpoint  \n",
    "\n",
    "Ap√≥s a conclus√£o do treinamento, podemos implantar o modelo treinado como um endpoint hospedado em realtime do Amazon SageMaker. Isso nos permitir√° fazer predi√ß√µes (ou scoring/infer√™ncia) a partir do modelo. Observe que n√£o precisamos hospedar no mesmo tipo de inst√¢ncia que costum√°vamos treinar. Como os pontos de extremidade da inst√¢ncia estar√£o em funcionamento por muito tempo, podemoos escolher uma inst√¢ncia mais barata para infer√™ncia.\n",
    "\n",
    "Este processo deve demorar de **4 a 8 minutos**... podemos acompanhar üòä pelo portal tamb√©m :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_endpoint = bt_model.deploy(initial_instance_count = 1,instance_type = 'ml.m4.xlarge')\n",
    "print(\"Conclu√≠do!\")\n",
    "print(\"EndpointName do modelo: {0}\".format( bt_endpoint.endpoint) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtendo representa√ß√µes vetoriais para palavras [incluindo out-of-vocabulary (OOV) words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como treinamos com o par√¢metro ** ```subwords =\"True\"``` **, podemos obter representa√ß√µes vetoriais para qualquer palavra - incluindo palavras com erros ortogr√°ficos ou palavras que n√£o estavam presentes no conjunto de dados de treinamento.\n",
    "Se treinarmos sem o sinalizador de subpalavras, o treinamento ser√° muito mais r√°pido, mas o modelo n√£o poder√° gerar vetores para palavras OOV. Em vez disso, retornar√° um vetor de zeros para essas palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use o padr√£o JSON para predi√ß√£o  \n",
    "\n",
    "O payload deve conter uma lista de palavras com a chave como \"**instances**\". O AWS Sagemaker suporta o tipo de conte√∫do `application/json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [\"awesome\", \"awweeesome\"]\n",
    "\n",
    "payload = {\"instances\" : words}\n",
    "\n",
    "response = bt_endpoint.predict(json.dumps(payload))\n",
    "\n",
    "vecs = json.loads(response)\n",
    "print(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( len(vecs) )\n",
    "print( len(vecs[0]['vector']) )\n",
    "print( len(vecs[1]['vector']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como esperado, obtemos um vetor n-dimensional (onde n √© vector_dim, conforme especificado nos hiperpar√¢metros) para cada uma das palavras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Ok! Agora √© avaliar se est√° funcionando a partir de outras chamadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
